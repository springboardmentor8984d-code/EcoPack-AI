# ğŸŒ± EcoPackAI: Intelligent Predictive Systems for Sustainable Logistics

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-Backend-black?style=for-the-badge&logo=flask&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Database-336791?style=for-the-badge&logo=postgresql&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-Machine%20Learning-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![XGBoost](https://img.shields.io/badge/XGBoost-Regression-red?style=for-the-badge)

**EcoPackAI** is an end-to-end Machine Learning control center designed to autonomously optimize enterprise packaging decisions. By analyzing product fragility, dimensional weights, and complex logistics routes, the system utilizes advanced dual-regression models to predict the exact financial cost and COâ‚‚ footprint of over 50 different packaging materials in real-time.

---

## ğŸ“– 1. Project Overview & Problem Statement

The modern supply chain operates under a critical dual mandate: reducing extreme logistical costs and aggressively minimizing single-use plastic waste. Traditional packaging selection relies heavily on static, rule-based heuristics (*"use bubble wrap for fragile items"*), leading to systemic **over-packaging** (inflating costs and COâ‚‚) or **under-packaging** (risking product damage). 

**EcoPackAI solves this by replacing human intuition with data-driven inference.** It predicts, scores, and ranks packaging materials dynamically, streaming this telemetry to an asynchronous Business Intelligence (BI) dashboard for enterprise stakeholders.

---

## âœ¨ 2. Core Features

* **ğŸ§  Dual-Model Inference Engine:** Utilizes a **Random Forest Regressor** for economic predictions (Cost) and an **XGBoost Regressor** for environmental predictions (COâ‚‚).
* **âš™ï¸ Implicit Feature Learning:** The models natively learned the complex physics of logistics. By training on a deterministic dataset, the AI automatically applies severe predictive penalties for structurally deficient packaging without requiring slow, hardcoded backend rules.
* **âš–ï¸ Dynamic AI Ranking Algorithm:** Applies Min-Max normalization and user-defined sustainability priorities (e.g., "High Sustainability" vs. "Cost Savings") to calculate a final **AI Match %**.
* **ğŸ“Š Real-Time BI Dashboard:** A Plotly-powered interface utilizing JavaScript polling (`setInterval`) to render live Material Adoption and Emission Trend charts without page refreshes.
* **ğŸ—„ï¸ Dynamic Database Integration:** Connected to PostgreSQL via SQLAlchemy, ensuring the frontend Predictor UI always pulls the latest live catalog of products and shipping routes.

---
## Cost Model File (Large File)

The trained cost model file is not included in this repository due to GitHub file size limitations (70MB).

Download the model from the link below:

Google Drive Link:
<https://drive.google.com/file/d/1BpsIeLJS957YpMM4TaA17_-oawO0put9/view?usp=sharing>

After downloading, place the file inside the following folder:
**/artifacts**

Make sure the file name is:
cost_model.pkl
---
## ğŸ“¸ 3. Application Screenshots

### The Predictor Interface
> *The AI control center where users input logistics parameters and receive real-time material rankings.*
![Predictor UI](Images/predictor_ui.png)

### Real-Time BI Dashboard
> *Live telemetry updating every 3 seconds, showing total COâ‚‚ saved, cost reductions, and dynamic material adoption charts.*
![Live Dashboard](Images/live_dashboard.png)
---
## ğŸ—ï¸ System Architecture

The application is decoupled into an API-driven architecture, ensuring the machine learning inference operates seamlessly with real-time UI updates.

```mermaid
graph TD
    A[Frontend: Predictor UI] -->|POST /api/v1/recommend| B(Flask Backend API)
    C[(PostgreSQL DB)] <-->|SQLAlchemy Fetch| B
    B -->|User Input| D[preprocessor.pkl]
    D --> E((Random Forest))
    D --> F((XGBoost))
    E -->|Target Cost| G{Ranking Algorithm}
    F -->|Target CO2| G
    G -->|Top 3 Telemetry| H[(live_dashboard_data.csv)]
    G -->|JSON Response| A
    H <-->|JS Polling GET /api/v1/analytics| I[Frontend: Live BI Dashboard]
```
---
## ğŸ·ï¸ **Machine Learning Pipeline**

The models were trained on a deterministic, physics-based synthetic dataset of 22,500 logistics permutations. The pipeline automatically standardizes numerical features and one-hot encodes categorical routes.

ğŸ”© **flowchart**
    Data[(Raw Input)] --> Preprocess[ColumnTransformer<br>StandardScaler + OneHotEncoder]
    Preprocess --> RF[Random Forest<br>Estimator]
    Preprocess --> XGB[XGBoost<br>Estimator]
    RF --> Out1[Predicted Cost INR]
    XGB --> Out2[Predicted CO2 kg]

---
## ğŸ“ **Model Performance**
The algorithms successfully reverse-engineered the complex logistical formulas utilized during the data generation phase, they achieved perfect predictive accuracy:

| Target Model | Target Variable | RÂ² Score |
| :--- | :--- | :--- |
| **Random Forest** | target_cost (INR) | 1.0000 |
| **XGBoost** | target_co2 (kg) | 1.0000 |

---
### ğŸ“ **Project Structure**
```
EcoPackAI/
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ cost_model.pkl        # Trained Random Forest Model
â”‚   â”œâ”€â”€ co2_model.pkl         # Trained XGBoost Model
|   â”œâ”€â”€ evaluation_metrics    # Evaluation Metrics of models
â”‚   â””â”€â”€ preprocessor.pkl      # Scaler & Encoder Pipeline
â”œâ”€â”€ static/
â”‚   â””â”€â”€ css/
â”‚       â””â”€â”€ style.css         # UI Styling
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html            # Predictor Interface
â”‚   â””â”€â”€ dashboard.html        # Live BI Dashboard
â”œâ”€â”€ raw_data/
â”‚   â””â”€â”€ unified_master_dataset.csv    # 22,500 Synthetic Scenarios
â”‚   â””â”€â”€ eda_summary_statistics        # Statistics of the Dataset
â”‚   â””â”€â”€ Packaging_materials.csv       # Ecofriendly materials dataset
â”‚   â””â”€â”€ Product_data.csv              # Product dataset
â”‚   â””â”€â”€ Shipping_data.csv             # Shipping dataset
â”œâ”€â”€ app.py                    # Main Flask Application
â”œâ”€â”€ EcoPackAI_Documentation   # Documentation
â”œâ”€â”€ live_dashboard_data.csv   # Real-time updated prediction data
â”œâ”€â”€ EcopackAI 2.ipynb         # Jupyter Notebook file
â”œâ”€â”€ requirements.txt          # Python Dependencies
â””â”€â”€ README.md                 # Project Documentation
```
---
## ğŸ› ï¸ Technology Stack
| Component | Technology | Description |
| :--- | :--- | :--- |
| **Backend Framework** | Python 3.9, Flask | Stateless REST API handling routing and ML inference. |
| **Database** | PostgreSQL, SQLAlchemy | Relational storage for dynamic UI population. |
| **Machine Learning** | Scikit-Learn, XGBoost | Feature preprocessing, regression, and data manipulation. |
| **Frontend UI** | HTML5, Bootstrap 5, Vanilla JS | Responsive predictor interface and dashboard layout. |
| **Data Visualization** | Plotly.js | Interactive, asynchronous charting engine. |
| **Reporting/Export** | `html2pdf.js`, `xlsxwriter` | 1-click PDF reports and Excel telemetry exports. |
---
## âš™ï¸ **Getting Started**
### Prerequisites

Python 3.9+

PostgreSQL installed and running on port 5432.

### Bash

git clone [https://github.com/CHERRY0456/EcoPackAI.git](https://github.com/CHERRY0456/EcoPackAI.git)

cd EcoPackAI

---
## Database Configuration
Open pgAdmin or your SQL CLI.

Create a new database named ecopack_db.

Import the CSV files (product_dataset.csv, shipping_dataset.csv, packaging_materials.csv) to create the base tables.

Update the DB_URI string in app.py with your PostgreSQL password:

**DB_URI = "postgresql+psycopg2://postgres:YOUR_PASSWORD@localhost:5432/ecopack_db"**

---
## ğŸ“ƒ Future Roadmap
Live Vendor APIs: Replace static baseline material pricing with real-time market API fetches from global packaging suppliers.

Computer Vision Edge Integration: Implement a Convolutional Neural Network (CNN) to auto-detect product dimensions and fragility from a physical camera feed on the warehouse floor.

Cloud Native Deployment: Containerize the application via Docker and deploy the ML inference engine to AWS SageMaker for global scalability.

---
## ğŸ§‘â€ğŸ’» Developed By: V. Jai Sri Charan
