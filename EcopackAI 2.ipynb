{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624d155-f88f-4f1b-b2ca-876baa3e5d7b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Initiating Master Dataset Preparation...\n",
      "‚úÖ Generated 22500 unique scenarios.\n",
      "üßÆ Calculating realistic Cost & CO‚ÇÇ using dimensions and rules...\n",
      "\n",
      "üéâ SUCCESS! The master dataset has been saved to: unified_scenarios_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "#Module 1 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚öôÔ∏è Initiating Master Dataset Preparation...\")\n",
    "\n",
    "# 1. LOAD THE RAW DATASETS\n",
    "try:\n",
    "    materials = pd.read_csv('packaging_materials.csv')\n",
    "    products = pd.read_csv('product_dataset.csv')\n",
    "    shipping = pd.read_csv('shipping_dataset.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: Could not find dataset files. Ensure they are in the same folder.\\n{e}\")\n",
    "    exit()\n",
    "\n",
    "for df in [materials, products, shipping]:\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "# 2. CROSS-JOIN (Create every possible scenario)\n",
    "# This creates a row for every material, for every product, via every shipping method.\n",
    "materials['key'] = 1\n",
    "products['key'] = 1\n",
    "shipping['key'] = 1\n",
    "unified_df = pd.merge(products, materials, on='key').merge(shipping, on='key')\n",
    "unified_df.drop('key', axis=1, inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(unified_df)} unique scenarios.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. REAL-WORLD LOGISTICS CALCULATIONS\n",
    "# ==========================================\n",
    "print(\"üßÆ Calculating realistic Cost & CO‚ÇÇ using dimensions and rules...\")\n",
    "\n",
    "# --- A. Dimensional Packaging Math ---\n",
    "# Surface area of the product box = 2(LW + WH + HL) / 10,000 to convert sq cm to sq meters\n",
    "unified_df['surface_area_m2'] = (2 * (\n",
    "    (unified_df['length_cm'] * unified_df['width_cm']) + \n",
    "    (unified_df['width_cm'] * unified_df['height_cm']) + \n",
    "    (unified_df['height_cm'] * unified_df['length_cm'])\n",
    ") / 10000) * 1.25 # 25% extra for folds, overlaps, and protective padding\n",
    "\n",
    "# How many layers of this material do we need to hold this product's weight?\n",
    "unified_df['layers_needed'] = np.ceil(unified_df['avg_weight'] / unified_df['weight_capacity']).clip(lower=1)\n",
    "unified_df['total_material_units'] = unified_df['surface_area_m2'] * unified_df['layers_needed']\n",
    "\n",
    "\n",
    "# --- B. Shipping Mode Physics ---\n",
    "def calculate_shipping_rates(row):\n",
    "    mode = str(row['shipping_type']).lower()\n",
    "    # Rates format: [Cost per kg per km, CO2 per kg per km]\n",
    "    if \"air\" in mode or \"express\" in mode or \"same day\" in mode:\n",
    "        return pd.Series([0.12, 0.008])  # Expensive & High CO2\n",
    "    elif \"cold\" in mode or \"refrigerated\" in mode or \"frozen\" in mode:\n",
    "        return pd.Series([0.08, 0.006])  # High energy required\n",
    "    elif \"city\" in mode or \"local\" in mode:\n",
    "        return pd.Series([0.05, 0.003])  # Stop-and-go traffic\n",
    "    elif \"bulk\" in mode or \"international\" in mode:\n",
    "        return pd.Series([0.02, 0.0015]) # Economies of scale\n",
    "    else:\n",
    "        return pd.Series([0.04, 0.002])  # Standard National/Regional transport\n",
    "\n",
    "unified_df[['ship_cost_rate', 'ship_co2_rate']] = unified_df.apply(calculate_shipping_rates, axis=1)\n",
    "\n",
    "\n",
    "# --- C. Industry Specific Rules ---\n",
    "def apply_industry_rules(row):\n",
    "    industry = str(row['industry_type']).lower()\n",
    "    if industry == \"electronics\":\n",
    "        return 1.4  # Requires extra anti-static bubble wrap/foam\n",
    "    elif industry in [\"food\", \"pharma\", \"cosmetics\"]:\n",
    "        return 1.3  # Requires thermal/hygienic liners\n",
    "    return 1.0\n",
    "\n",
    "unified_df['industry_multiplier'] = unified_df.apply(apply_industry_rules, axis=1)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. FINAL GROUND TRUTH GENERATION\n",
    "# ==========================================\n",
    "\n",
    "# Final Cost = (Packaging Material) + (Transport: Weight x Distance x Rate) + (Risk Penalty)\n",
    "unified_df['target_cost'] = (\n",
    "    (unified_df['cost_per_unit'] * unified_df['total_material_units'] * unified_df['industry_multiplier']) +\n",
    "    (unified_df['avg_weight'] * unified_df['distance_km'] * unified_df['ship_cost_rate']) +\n",
    "    (unified_df['handling_risk'] * unified_df['fragility_level'] * 2.5) # Penalty for breaking fragile items\n",
    ").round(2)\n",
    "\n",
    "# Final CO2 = (Packaging Manufacturing) + (Transport Emission: Weight x Distance x Rate)\n",
    "unified_df['target_co2'] = (\n",
    "    (unified_df['co2_emission_score'] * unified_df['total_material_units']) +\n",
    "    (unified_df['avg_weight'] * unified_df['distance_km'] * unified_df['ship_co2_rate'])\n",
    ").round(2)\n",
    "\n",
    "# Add extreme penalty for highly unrealistic scenarios (e.g. putting a 10kg item in a 1kg capacity bag)\n",
    "unified_df.loc[unified_df['avg_weight'] > (unified_df['weight_capacity'] * 3), 'target_cost'] += 5000  \n",
    "\n",
    "# 5. SAVE THE PERFECT DATASET\n",
    "output_file = 'unified_scenarios_dataset.csv'\n",
    "unified_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nüéâ SUCCESS! The master dataset has been saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d376f1d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Initiating Exploratory Data Analysis (EDA) for 'unified_scenarios_dataset.csv'...\n",
      "\n",
      "=== 1. DATASET SHAPE ===\n",
      "‚û§ Total Records (Rows): 22,500\n",
      "‚û§ Total Features (Columns): 30\n",
      "\n",
      "=== 2. FEATURE CLASSIFICATION ===\n",
      "‚û§ Numerical Features (26): avg_weight, fragility_level, shelf_life_days, moisture_sensitivity, chemical_sensitivity ...\n",
      "‚û§ Categorical Features (4): product_name, industry_type, material_type, shipping_type ...\n",
      "\n",
      "=== 3. DATA CLEANLINESS (MISSING VALUES) ===\n",
      "‚úÖ Outstanding! The dataset is perfectly clean with ZERO missing values.\n",
      "\n",
      "=== 4. DUPLICATE RECORDS ===\n",
      "‚úÖ No duplicate rows found.\n",
      "\n",
      "=== 5. STATISTICAL SUMMARY ===\n",
      "       avg_weight  fragility_level  shelf_life_days  moisture_sensitivity\n",
      "count    22500.00         22500.00         22500.00              22500.00\n",
      "mean         0.91             5.27          1603.23                  4.90\n",
      "std          1.02             2.22          1498.86                  2.86\n",
      "min          0.10             2.00             7.00                  1.00\n",
      "25%          0.30             3.00           365.00                  2.00\n",
      "50%          0.50             5.00           912.50                  5.00\n",
      "75%          1.00             7.00          3650.00                  7.00\n",
      "max          5.00             9.00          3650.00                 10.00\n",
      "\n",
      "üìÅ SUCCESS: Full EDA statistical report saved to 'eda_summary_statistics.csv'!\n",
      "üéØ You can submit this CSV directly to your mentor.\n"
     ]
    }
   ],
   "source": [
    "#Module 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def perform_eda(file_name='unified_scenarios_dataset.csv', output_name='eda_summary_statistics.csv'):\n",
    "    print(f\"üîç Initiating Exploratory Data Analysis (EDA) for '{file_name}'...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Could not find '{file_name}'. Make sure it's in the same folder!\")\n",
    "        return\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. DATASET SHAPE & STRUCTURE\n",
    "    # ==========================================\n",
    "    print(\"=== 1. DATASET SHAPE ===\")\n",
    "    print(f\"‚û§ Total Records (Rows): {df.shape[0]:,}\")\n",
    "    print(f\"‚û§ Total Features (Columns): {df.shape[1]}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. DATA TYPES & FEATURE CLASSIFICATION\n",
    "    # ==========================================\n",
    "    print(\"\\n=== 2. FEATURE CLASSIFICATION ===\")\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"‚û§ Numerical Features ({len(num_cols)}): {', '.join(num_cols[:5])} ...\")\n",
    "    print(f\"‚û§ Categorical Features ({len(cat_cols)}): {', '.join(cat_cols[:5])} ...\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. MISSING VALUES CHECK (Crucial for Mentors)\n",
    "    # ==========================================\n",
    "    print(\"\\n=== 3. DATA CLEANLINESS (MISSING VALUES) ===\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_cols = missing_data[missing_data > 0]\n",
    "    \n",
    "    if missing_cols.empty:\n",
    "        print(\"‚úÖ Outstanding! The dataset is perfectly clean with ZERO missing values.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Missing values detected in the following columns:\")\n",
    "        for col, count in missing_cols.items():\n",
    "            print(f\"   - {col}: {count} missing values\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. DUPLICATE CHECK\n",
    "    # ==========================================\n",
    "    print(\"\\n=== 4. DUPLICATE RECORDS ===\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates == 0:\n",
    "        print(\"‚úÖ No duplicate rows found.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Found {duplicates} duplicate rows.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. GENERATE & SAVE SUMMARY STATISTICS\n",
    "    # ==========================================\n",
    "    print(\"\\n=== 5. STATISTICAL SUMMARY ===\")\n",
    "    # Calculate count, mean, std, min, 25%, 50%, 75%, max for all numbers\n",
    "    summary_stats = df.describe().round(2)\n",
    "    \n",
    "    # Print a small preview of the first 4 columns to the terminal\n",
    "    print(summary_stats.iloc[:, :4]) \n",
    "    \n",
    "    # Save the full report to CSV for your submission\n",
    "    summary_stats.to_csv(output_name)\n",
    "    print(f\"\\nüìÅ SUCCESS: Full EDA statistical report saved to '{output_name}'!\")\n",
    "    print(\"üéØ You can submit this CSV directly to your mentor.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    perform_eda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3a1c8-18a1-412f-98d4-cf421ec324b2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Initiating Module 3: ML Dataset Preparation...\n",
      "üîÄ Splitting 22500 records into Training and Testing sets...\n",
      "   ‚û§ Training Set: 18000 rows\n",
      "   ‚û§ Testing Set:  4500 rows\n",
      "‚öñÔ∏è Building Scaling and Encoding Pipelines...\n",
      "‚úÖ Module 3 Complete! Data is split, scaled, encoded, and ready for Model Training.\n"
     ]
    }
   ],
   "source": [
    "#Module 3\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "print(\"‚öôÔ∏è Initiating Module 3: ML Dataset Preparation...\")\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD THE TARGET-READY DATASET\n",
    "# ==========================================\n",
    "# This dataset already has our perfectly calculated 'target_cost' and 'target_co2'\n",
    "df = pd.read_csv('unified_scenarios_dataset.csv')\n",
    "\n",
    "# ==========================================\n",
    "# 2. SELECT ML FEATURES FOR PREDICTION\n",
    "# ==========================================\n",
    "# We isolate the 13 physical and categorical features the AI is allowed to learn from.\n",
    "features = [\n",
    "    'strength', 'cost_per_unit', 'co2_emission_score', 'weight_capacity', \n",
    "    'avg_weight', 'fragility_level', 'distance_km', 'handling_risk',\n",
    "    'length_cm', 'width_cm', 'height_cm', 'shipping_type', 'industry_type'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y_cost = df['target_cost']\n",
    "y_co2 = df['target_co2']\n",
    "\n",
    "# ==========================================\n",
    "# 3. SPLIT DATA INTO TRAINING (80%) AND TESTING (20%)\n",
    "# ==========================================\n",
    "print(f\"üîÄ Splitting {len(df)} records into Training and Testing sets...\")\n",
    "\n",
    "# Split for Cost Prediction\n",
    "X_train, X_test, y_cost_train, y_cost_test = train_test_split(X, y_cost, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split for CO2 Prediction (Reusing the same X split to ensure consistency)\n",
    "_, _, y_co2_train, y_co2_test = train_test_split(X, y_co2, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"   ‚û§ Training Set: {len(X_train)} rows\")\n",
    "print(f\"   ‚û§ Testing Set:  {len(X_test)} rows\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREPARE DATA PIPELINES & SCALING\n",
    "# ==========================================\n",
    "print(\"‚öñÔ∏è Building Scaling and Encoding Pipelines...\")\n",
    "\n",
    "# Identify numerical vs categorical columns\n",
    "num_cols = [\n",
    "    'strength', 'cost_per_unit', 'co2_emission_score', 'weight_capacity', \n",
    "    'avg_weight', 'fragility_level', 'distance_km', 'handling_risk', \n",
    "    'length_cm', 'width_cm', 'height_cm'\n",
    "]\n",
    "cat_cols = ['shipping_type', 'industry_type']\n",
    "\n",
    "# Build the ColumnTransformer Pipeline\n",
    "# - StandardScaler: Normalizes continuous numbers (e.g., distance, weight) to mean=0, std=1\n",
    "# - OneHotEncoder: Converts categories (e.g., 'Air Cargo') into binary 1s and 0s\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "])\n",
    "\n",
    "# FIT AND TRANSFORM the training data, but ONLY TRANSFORM the testing data (Prevents Data Leakage)\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 5. SAVE ARTIFACTS FOR MODULE 4\n",
    "# ==========================================\n",
    "# Saving the fitted preprocessor is crucial so app.py scales new user inputs the exact same way!\n",
    "joblib.dump(preprocessor, 'artifacts/preprocessor.pkl')\n",
    "\n",
    "print(\"‚úÖ Module 3 Complete! Data is split, scaled, encoded, and ready for Model Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551519d-f58c-4c2d-8096-01d55dd66b45",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Initiating Module 4: ML Model Training & Evaluation...\n",
      "üì¶ Loading Unified Scenario Dataset...\n",
      "\n",
      "üß† Training Random Forest (Cost Prediction Model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random Forest Epochs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10<00:00,  9.92tree/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå± Training XGBoost (CO‚ÇÇ Prediction Model)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGBoost Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.57model/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Random Forest (Cost) Performance:\n",
      "   ‚û§ MAE:  ¬±0.09\n",
      "   ‚û§ RMSE: 0.25\n",
      "   ‚û§ R¬≤:   1.0000\n",
      "\n",
      "üìä XGBoost (CO‚ÇÇ) Performance:\n",
      "   ‚û§ MAE:  ¬±0.02\n",
      "   ‚û§ RMSE: 0.03\n",
      "   ‚û§ R¬≤:   1.0000\n",
      "\n",
      "‚úÖ Models and Metrics saved successfully to the 'artifacts' folder.\n",
      "\n",
      "üèÜ TESTING THE ML-BASED RANKING SYSTEM...\n",
      "Scenario: Shipping a 0.4kg item via Regional Transport\n",
      "\n",
      "--- AI Recommended Material Rankings ---\n",
      "             Material  Pred Cost (‚Çπ)  Pred CO‚ÇÇ (kg)  Waste Penalty  AI Score\n",
      "Rank                                                                        \n",
      "1     Recycled Mailer          95.88           0.49           0.07    0.0398\n",
      "2         Molded Pulp          95.89           0.49           0.19    0.0645\n",
      "3      Corrugated Box          96.00           0.54           0.33    0.1911\n",
      "4        Bamboo Crate          97.13           0.49           1.00    0.5000\n",
      "5     Plastic Polybag          95.91           0.80           0.00    0.5088\n"
     ]
    }
   ],
   "source": [
    "#Module 4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"‚öôÔ∏è Initiating Module 4: ML Model Training & Evaluation...\")\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD & PREPARE DATA (From Module 3)\n",
    "# ==========================================\n",
    "print(\"üì¶ Loading Unified Scenario Dataset...\")\n",
    "df = pd.read_csv('unified_scenarios_dataset.csv')\n",
    "\n",
    "features = [\n",
    "    'strength', 'cost_per_unit', 'co2_emission_score', 'weight_capacity', \n",
    "    'avg_weight', 'fragility_level', 'distance_km', 'handling_risk',\n",
    "    'length_cm', 'width_cm', 'height_cm', 'shipping_type', 'industry_type'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y_cost = df['target_cost']\n",
    "y_co2 = df['target_co2']\n",
    "\n",
    "X_train, X_test, yc_train, yc_test = train_test_split(X, y_cost, test_size=0.2, random_state=42)\n",
    "_, _, ye_train, ye_test = train_test_split(X, y_co2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline Definition\n",
    "num_cols = ['strength', 'cost_per_unit', 'co2_emission_score', 'weight_capacity', 'avg_weight', \n",
    "            'fragility_level', 'distance_km', 'handling_risk', 'length_cm', 'width_cm', 'height_cm']\n",
    "cat_cols = ['shipping_type', 'industry_type']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "])\n",
    "\n",
    "X_train_p = preprocessor.fit_transform(X_train)\n",
    "X_test_p = preprocessor.transform(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 2. TRAIN MODELS WITH TQDM PROGRESS BARS\n",
    "# ==========================================\n",
    "N_ESTIMATORS = 100\n",
    "\n",
    "print(\"\\nüß† Training Random Forest (Cost Prediction Model)...\")\n",
    "# Using warm_start to iteratively add trees and update TQDM\n",
    "cost_model = RandomForestRegressor(n_estimators=1, warm_start=True, random_state=42, n_jobs=-1)\n",
    "\n",
    "with tqdm(total=N_ESTIMATORS, desc=\"Random Forest Epochs\", unit=\"tree\") as pbar:\n",
    "    for i in range(1, N_ESTIMATORS + 1):\n",
    "        cost_model.n_estimators = i\n",
    "        cost_model.fit(X_train_p, yc_train)\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"\\nüå± Training XGBoost (CO‚ÇÇ Prediction Model)...\")\n",
    "# Bulletproof XGBoost Training\n",
    "co2_model = XGBRegressor(n_estimators=N_ESTIMATORS, learning_rate=0.1, max_depth=8, random_state=42)\n",
    "\n",
    "with tqdm(total=1, desc=\"XGBoost Training\", unit=\"model\") as pbar:\n",
    "    co2_model.fit(X_train_p, ye_train)\n",
    "    pbar.update(1)\n",
    "\n",
    "# ==========================================\n",
    "# 3. EVALUATION METRICS (RMSE, MAE, R¬≤)\n",
    "# ==========================================\n",
    "def evaluate(y_true, y_pred, name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä {name} Performance:\")\n",
    "    print(f\"   ‚û§ MAE:  ¬±{mae:.2f}\")\n",
    "    print(f\"   ‚û§ RMSE: {rmse:.2f}\")\n",
    "    print(f\"   ‚û§ R¬≤:   {r2:.4f}\")\n",
    "    return {\"MAE\": round(mae, 4), \"RMSE\": round(rmse, 4), \"R2\": round(r2, 4)}\n",
    "\n",
    "metrics = {\n",
    "    \"Cost_Prediction_RF\": evaluate(yc_test, cost_model.predict(X_test_p), \"Random Forest (Cost)\"),\n",
    "    \"CO2_Prediction_XGB\": evaluate(ye_test, co2_model.predict(X_test_p), \"XGBoost (CO‚ÇÇ)\")\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 4. SAVE ARTIFACTS\n",
    "# ==========================================\n",
    "joblib.dump(preprocessor, 'artifacts/preprocessor.pkl')\n",
    "joblib.dump(cost_model, 'artifacts/cost_model.pkl')\n",
    "joblib.dump(co2_model, 'artifacts/co2_model.pkl')\n",
    "\n",
    "with open('artifacts/evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ Models and Metrics saved successfully to the 'artifacts' folder.\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. ML-BASED MATERIAL RANKING SYSTEM\n",
    "# ==========================================\n",
    "# ==========================================\n",
    "# 5. ML-BASED MATERIAL RANKING SYSTEM (Upgraded)\n",
    "# ==========================================\n",
    "print(\"\\nüèÜ TESTING THE ML-BASED RANKING SYSTEM...\")\n",
    "def rank_materials_for_scenario(scenario_index=0):\n",
    "    # 1. Grab a sample product & shipping configuration\n",
    "    sample_df = X_test.iloc[[scenario_index]].copy()\n",
    "    prod_weight = sample_df['avg_weight'].values[0]\n",
    "    print(f\"Scenario: Shipping a {prod_weight}kg item via {sample_df['shipping_type'].values[0]}\")\n",
    "\n",
    "    # 2. Simulate 5 different materials\n",
    "    simulation_df = pd.concat([sample_df]*5, ignore_index=True)\n",
    "    simulation_materials = ['Corrugated Box', 'Bamboo Crate', 'Recycled Mailer', 'Molded Pulp', 'Plastic Polybag']\n",
    "    simulation_df['strength'] = [6, 9, 4, 5, 2]\n",
    "    simulation_df['cost_per_unit'] = [8.0, 25.0, 3.5, 6.0, 1.0]\n",
    "    simulation_df['co2_emission_score'] = [3, 1, 2, 2, 9]\n",
    "    simulation_df['weight_capacity'] = [12, 30, 5, 8, 3]\n",
    "\n",
    "    # 3. Predict Cost and CO2\n",
    "    sim_processed = preprocessor.transform(simulation_df)\n",
    "    pred_costs = cost_model.predict(sim_processed)\n",
    "    pred_co2s = co2_model.predict(sim_processed)\n",
    "\n",
    "    # 4. ADVANCED RANKING LOGIC\n",
    "    def normalize(arr): \n",
    "        return (arr - np.min(arr)) / (np.max(arr) - np.min(arr) + 1e-9)\n",
    "    \n",
    "    cost_scores = normalize(pred_costs)\n",
    "    co2_scores = normalize(pred_co2s)\n",
    "    \n",
    "    # Calculate Over-packaging / Waste Penalty\n",
    "    # If a material holds 30kg but the item is 0.3kg, penalty is huge!\n",
    "    waste_ratio = simulation_df['weight_capacity'] / prod_weight\n",
    "    waste_penalty = normalize(waste_ratio)\n",
    "\n",
    "    # Material Eco-Friendliness (Inherent CO2 score of the material itself)\n",
    "    eco_scores = normalize(simulation_df['co2_emission_score'])\n",
    "\n",
    "    # Final Smart Score (Lower is better)\n",
    "    # 30% Route Cost + 30% Route CO2 + 20% Waste Penalty + 20% Inherent Material Eco-Score\n",
    "    final_scores = (cost_scores * 0.30) + (co2_scores * 0.30) + (waste_penalty * 0.20) + (eco_scores * 0.20)\n",
    "\n",
    "    # 5. Output the Ranked List\n",
    "    results = pd.DataFrame({\n",
    "        'Material': simulation_materials,\n",
    "        'Pred Cost (‚Çπ)': np.round(pred_costs, 2),\n",
    "        'Pred CO‚ÇÇ (kg)': np.round(pred_co2s, 2),\n",
    "        'Waste Penalty': np.round(waste_penalty, 2), # Showing the penalty for clarity\n",
    "        'AI Score': np.round(final_scores, 4)\n",
    "    }).sort_values('AI Score', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    results.index += 1 \n",
    "    results.index.name = 'Rank'\n",
    "    print(\"\\n--- AI Recommended Material Rankings ---\")\n",
    "    print(results)\n",
    "\n",
    "rank_materials_for_scenario(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d65f19-82bc-4646-908b-54e4675f323f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "R",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
